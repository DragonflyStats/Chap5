\documentclass[main.tex]{subfiles}
\begin{document}
	
	%---------------------------------------------------------------------------%
	\newpage
	\section{Residual diagnostics} %1.3
	For classical linear models, residual diagnostics are typically implemented as a plot of the observed residuals and the predicted values. A visual inspection for the presence of trends inform the analyst on the validity of distributional assumptions, and to detect outliers and influential observations.
	
	
	
	%--Marginal and Conditional Residuals
	
	\subsection{Residuals diagnostics in mixed models}
	
	%schabenberger
	The marginal and conditional means in the linear mixed model are
	$E[\boldsymbol{Y}] = \boldsymbol{X}\boldsymbol{\beta}$ and
	$E[\boldsymbol{Y|\boldsymbol{u}}] = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{Z}\boldsymbol{u}$, respectively.
	
	A residual is the difference between an observed quantity and its estimated or predicted value. In the mixed
	model you can distinguish marginal residuals $r_m$ and conditional residuals $r_c$. 
	
	
	\subsection{Marginal and Conditional Residuals}
	
	A marginal residual is the difference between the observed data and the estimated (marginal) mean, $r_{mi} = y_i - x_0^{\prime} \hat{b}$
	A conditional residual is the difference between the observed data and the predicted value of the observation,
	$r_{ci} = y_i - x_i^{\prime} \hat{b} - z_i^{\prime} \hat{\gamma}$
	
	In linear mixed effects models, diagnostic techniques may consider `conditional' residuals. A conditional residual is the difference between an observed value $y_{i}$ and the conditional predicted value $\hat{y}_{i} $.
	
	\[ \hat{epsilon}_{i} = y_{i} - \hat{y}_{i} = y_{i} - ( X_{i}\hat{beta} + Z_{i}\hat{b}_{i}) \]
	
	However, using conditional residuals for diagnostics presents difficulties, as they tend to be correlated and their variances may be different for different subgroups, which can lead to erroneous conclusions.
	
	%1.5
	%http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_mixed_sect024.htm
	
	
	
	
	
	
	\begin{equation}
	r_{mi}=x^{T}_{i}\hat{\beta}
	\end{equation}
	
	\subsection{Marginal Residuals}
	\begin{eqnarray}
	\hat{\beta} &=& (X^{T}R^{-1}X)^{-1}X^{T}R^{-1}Y \nonumber \\
	&=& BY \nonumber
	\end{eqnarray}
	
	
\section{Conditional and Marginal Residuals}
Conditional residuals include contributions from both fixed and random effects, whereas marginal residuals include contribution from only fixed effects.

Suppose the linear mixed-effects model lme has an $n \times p$ fixed-effects design matrix $\boldsymbol{X}$ and an $n \times q$ random-effects design matrix $\boldsymbol{Z}$. 

Also, suppose the p-by-1 estimated fixed-effects vector is $\hat{\beta}$ , and the q-by-1 estimated best linear unbiased predictor (BLUP) 
vector of random effects is $\hat{b}$ . The fitted conditional response is

\[ \hat{y}_{Cond} = X \hat{\beta} + Z \hat{b} \]

and the fitted marginal response is


\[ \hat{y}_{Mar} = X \hat{\beta} \]

residuals can return three types of residuals:
\begin{itemize} 
\item raw, 
\item Pearson, and 
\item standardized.\end{itemize} For any type, you can compute the conditional or the marginal residuals. For example, the conditional raw residual is


\[ r_{Cond} = y - X \hat{\beta} - Z \hat{b} \]

and the marginal raw residual is



\[ r_{Mar} = y - X \hat{\beta} \]

\newpage
%=================================================== %
% http://www.ime.usp.br/~jmsinger/MAE0610/Mixedmodelresiduals.pdf

Cox and Snell (1968, JRSS-B): general definition of residuals for
models with single source of variability
Hilden-Minton (1995, PhD thesis UCLA), Verbeke and Lesaffre
(1997, CSDA) or Pinheiro and Bates (2000, Springer): extension to
define three types of residuals that accommodate the extra source of
variability present in linear mixed models, namely:

i) Marginal residuals, 
%bξ = y − X\hat{\beta} = \hat{M}^{-1}\hat{Q}y ,

predictors of marginal errors, 

%ξ = y − E[y] = y − X\beta = Zb + e

ii) Conditional residuals, 
\[be = y − X\hat{\beta} − Zbb = \hat{\sigma}Q\hat{y}\] , predictors of
conditional errors 
\[e = y − E[y|b] = y − X\beta − Zb\]

iii) BLUP, Zbb, predictors of random effects,
\[ Zb = E[y|b] − E[y]\]


%------------------------------------------------------------------%
\newpage

\subsection*{Marginal residuals}

\[y - X\beta = Z \eta +\epsilon \]
\begin{itemize}
\item
Should be mean 0, but may show grouping structure
\item
May not be homoskedastic.
\item
Good for checking fixed effects, just like linear regr.
\end{itemize}
%----------------------------------------------------%
\subsection*{Conditional residuals}
\[y - X\beta - Z \eta = \epsilon \]
\begin{itemize}
\item
Should be mean zero with no grouping structure
\item
Should be homoscedastic.
\item
Good for checking normality of outliers
\end{itemize}

%-----------------------------------------------------%
\subsection*{Random effects}
\[y - X\beta -\epsilon = Z \eta \]
\begin{itemize}
\item
Should be mean zero with no grouping structure
\item
May not be be homoscedastic.
\end{itemize}

\section{Residual diagnostics} %1.3
For classical linear models, residual diagnostics are typically implemented as a plot of the observed residuals and the predicted values. A visual inspection for the presence of trends inform the analyst on the validity of distributional assumptions, and to detect outliers and influential observations.



%--Marginal and Conditional Residuals

\subsection{Residuals diagnostics in mixed models}

%schabenberger
The marginal and conditional means in the linear mixed model are
$E[\boldsymbol{Y}] = \boldsymbol{X}\boldsymbol{\beta}$ and
$E[\boldsymbol{Y|\boldsymbol{u}}] = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{Z}\boldsymbol{u}$, respectively.

A residual is the difference between an observed quantity and its estimated or predicted value. In the mixed
model you can distinguish marginal residuals $r_m$ and conditional residuals $r_c$. 


\subsection{Marginal and Conditional Residuals}

A marginal residual is the difference between the observed data and the estimated (marginal) mean, $r_{mi} = y_i - x_0^{\prime} \hat{b}$
A conditional residual is the difference between the observed data and the predicted value of the observation,
$r_{ci} = y_i - x_i^{\prime} \hat{b} - z_i^{\prime} \hat{\gamma}$

In linear mixed effects models, diagnostic techniques may consider `conditional' residuals. A conditional residual is the difference between an observed value $y_{i}$ and the conditional predicted value $\hat{y}_{i} $.

\[ \hat{epsilon}_{i} = y_{i} - \hat{y}_{i} = y_{i} - ( X_{i}\hat{beta} + Z_{i}\hat{b}_{i}) \]

However, using conditional residuals for diagnostics presents difficulties, as they tend to be correlated and their variances may be different for different subgroups, which can lead to erroneous conclusions.

%1.5
%http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_mixed_sect024.htm






\begin{equation}
	r_{mi}=x^{T}_{i}\hat{\beta}
\end{equation}

\subsection{Marginal Residuals}
\begin{eqnarray}
	\hat{\beta} &=& (X^{T}R^{-1}X)^{-1}X^{T}R^{-1}Y \nonumber \\
	&=& BY \nonumber
\end{eqnarray}

%---------------------------------------------------------------------------%
\newpage
\section{Standardized and studentized residuals} %1.4
%--Studentized and Standardized Residuals

To alleviate the problem caused by inconstant variance, the residuals are scaled (i.e. divided) by their standard deviations. This results in a \index{standardized residual}`standardized residual'. Because true standard deviations are frequently unknown, one can instead divide a residual by the estimated standard deviation to obtain the \index{studentized residual}`studentized residual. 

\subsection{Standardization} %1.4.1

A random variable is said to be standardized if the difference from its mean is scaled by its standard deviation. The residuals above have mean zero but their variance is unknown, it depends on the true values of $\theta$. Standardization is thus not possible in practice.

\subsection{Studentization} %1.4.2
Instead, you can compute studentized residuals by dividing a residual by an estimate of its standard deviation. 

\subsection{Internal and External Studentization} %1.4.3
If that estimate is independent of the $i-$th observation, the process is termed \index{external studentization}`external studentization'. This is usually accomplished by excluding the $i-$th observation when computing the estimate of its standard error. If the observation contributes to the
standard error computation, the residual is said to be \index{internally studentization}internally studentized.

Externally \index{studentized residual} studentized residual require iterative influence analysis or a profiled residuals variance.


\subsection{Computation}%1.4.4

The computation of internally studentized residuals relies on the diagonal entries of $\boldsymbol{V} (\hat{\theta})$ - $\boldsymbol{Q} (\hat{\theta})$, where $\boldsymbol{Q} (\hat{\theta})$ is computed as

\[ \boldsymbol{Q} (\hat{\theta}) = \boldsymbol{X} ( \boldsymbol{X}^{\prime}\boldsymbol{Q} (\hat{\theta})^{-1}\boldsymbol{X})\boldsymbol{X}^{-1} \]

\subsection{Pearson Residual}%1.4.5

Another possible scaled residual is the \index{Pearson residual} `Pearson residual', whereby a residual is divided by the standard deviation of the dependent variable. The Pearson residual can be used when the variability of $\hat{\beta}$ is disregarded in the underlying assumptions.

%---------------------------------------------------------------------------%
\newpage
\section{Covariance Parameters} %1.5
The unknown variance elements are referred to as the covariance parameters and collected in the vector $\theta$.
% - where is this coming from?
% - where is it used again?
% - Has this got anything to do with CovTrace etc?
%---------------------------------------------------------------------------%



\subsection{Confounded Residuals}
Hilden-Minton (1995, PhD thesis, UCLA): residual is pure for a
specific type of error if it depends only on the fixed components and
on the error that it is supposed to predict
Residuals that depend on other types of errors are called \textit{\textbf{confounded
residuals}}

\end{document}
