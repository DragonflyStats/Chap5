\documentclass[12pt, a4paper]{report}
\usepackage{epsfig}
\usepackage{subfigure}
%\usepackage{amscd}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{amsthm}
%\usepackage[dvips]{graphicx}
\usepackage{natbib}
\usepackage{subfiles}
\bibliographystyle{chicago}
\usepackage{vmargin}
\usepackage{index}
% left top textwidth textheight headheight
% headsep footheight footskip
\setmargins{3.0cm}{2.5cm}{15.5 cm}{22cm}{0.5cm}{0cm}{1cm}{1cm}
\renewcommand{\baselinestretch}{1.5}
\pagenumbering{arabic}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{ill}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{axiom}{Axiom}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{notation}{Notation}
\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]
\renewcommand{\thenotation}{}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\title{Research notes: linear mixed effects models}
\author{ } \date{ }


\makeindex
\begin{document}
\author{Kevin O'Brien}
\title{November 2011 Version A}


\addcontentsline{toc}{section}{Bibliography}


\tableofcontents \setcounter{tocdepth}{1}


%---------------------------------------------------------------------------%
% - 1. Model Diagnostics
% - 2. Zewotir's paper (including Haslett)
% - 3. Augmented GLMS
% - 4. Applying Diagnostics to MCS
% - 5. Extra Material
%---------------------------------------------------------------------------%

%---------------------------------------------------------------------------%


%---------------------------------------------------------------------------%
\newpage
\section{Residual diagnostics} %1.3
For classical linear models, residual diagnostics are typically implemented as a plot of the observed residuals and the predicted values. A visual inspection for the presence of trends inform the analyst on the validity of distributional assumptions, and to detect outliers and influential observations.



	%--Marginal and Conditional Residuals
	
\subsection{Residuals diagnostics in mixed models}

%schabenberger
The marginal and conditional means in the linear mixed model are
$E[\boldsymbol{Y}] = \boldsymbol{X}\boldsymbol{\beta}$ and
$E[\boldsymbol{Y|\boldsymbol{u}}] = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{Z}\boldsymbol{u}$, respectively.

A residual is the difference between an observed quantity and its estimated or predicted value. In the mixed
model you can distinguish marginal residuals $r_m$ and conditional residuals $r_c$. 


\subsection{Marginal and Conditional Residuals}

A marginal residual is the difference between the observed data and the estimated (marginal) mean, $r_{mi} = y_i - x_0^{\prime} \hat{b}$
A conditional residual is the difference between the observed data and the predicted value of the observation,
$r_{ci} = y_i - x_i^{\prime} \hat{b} - z_i^{\prime} \hat{\gamma}$

In linear mixed effects models, diagnostic techniques may consider `conditional' residuals. A conditional residual is the difference between an observed value $y_{i}$ and the conditional predicted value $\hat{y}_{i} $.

\[ \hat{epsilon}_{i} = y_{i} - \hat{y}_{i} = y_{i} - ( X_{i}\hat{beta} + Z_{i}\hat{b}_{i}) \]

However, using conditional residuals for diagnostics presents difficulties, as they tend to be correlated and their variances may be different for different subgroups, which can lead to erroneous conclusions.

%1.5
%http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_mixed_sect024.htm









\chapter{Zewotir's Paper}


% 2.1 Efficient Updating Theorem
% 2.2 Zewotir Measures of Influence in LME Models (section 4 of paper)
% 2.3 Computation and Notation
% 2.4 Measures 2
% 2.5 Haslett Hayes Paper
% 2.6 Demidenko I Influence


\section{Efficient Updating Theorem} %2.1
\citet{Zewotir} describes the basic theorem of efficient updating.
\begin{itemize}
\item \[ m_i = {1 \over c_{ii}}\]
%\item
%item
%\item
\end{itemize}
%-------------------------------------------------------------------------------------------------------------------------------------%
\section{Zewotir Measures of Influence in LME Models}%2.2
%Zewotir page 161
\citet{Zewotir} describes a number of approaches to model diagnostics, investigating each of the following;
\begin{itemize}
\item Variance components
\item Fixed effects parameters
\item Prediction of the response variable and of random effects
\item likelihood function
\end{itemize}


\subsection{Cook's Distance}
\begin{itemize}
\item For variance components $\gamma$: $CD(\gamma)_i$,
\item For fixed effect parameters $\beta$: $CD(\beta)_i$,
\item For random effect parameters $\boldsymbol{u}$: $CD(u)_i$,
\item For linear functions of $\hat{beta}$: $CD(\psi)_i$
\end{itemize}


\newpage
\subsubsection{Random Effects}


A large value for $CD(u)_i$ indicates that the $i-$th observation is influential in predicting random effects.


\subsubsection{linear functions}


$CD(\psi)_i$ does not have to be calculated unless $CD(\beta)_i$ is large.




\subsection{Information Ratio}




%--------------------------------------------------------------%
\newpage
\section{Computation and Notation } %2.3
with $\boldsymbol{V}$ unknown, a standard practice for estimating $\boldsymbol{X \beta}$ is the estime the variance components $\sigma^2_j$,
compute an estimate for $\boldsymbol{V}$ and then compute the projector matrix $A$, $\boldsymbol{X \hat{\beta}}  = \boldsymbol{AY}$.




\citet{zewotir} remarks that $\boldsymbol{D}$ is a block diagonal with the $i-$th block being $u \boldsymbol{I}$

\newpage
\section{Haslett's Analysis} %2.5
For fixed effect linear models with correlated error structure Haslett (1999) showed that the effects on
the fixed effects estimate of deleting each observation in turn could be cheaply computed from the fixed effects model predicted residuals.


A general theory is presented for residuals from the general linear model with correlated errors.
It is demonstrated that there are two fundamental types of residual associated with this model,
referred to here as the marginal and the conditional residual.


These measure respectively the distance to the global aspects of the model as represented by the expected value
and the local aspects as represented by the conditional expected value.


These residuals may be multivariate.


\citet{HaslettHayes} developes some important dualities which have simple implications for diagnostics.


%The results are illustrated by reference to model diagnostics in time series and in classical multivariate analysis with independent cases.



\section{Demidenko's I Influence} %2.6
The concept of I Influence is generalized  to the non linea regression model.
%-------------------------------------------------------------------------------------------------------------------------------------%
%-------------------------------------------------------------------------------------------------------------------------------------%
%-------------------------------------------------------------------------------------------------Chapter 3------------------------%
%-------------------------------------------------------------------------------------------------------------------------------------%
%-------------------------------------------------------------------------------------------------------------------------------------%






\chapter{Application to Method Comparison Studies} % Chapter 4




%---------------------------------------------------------------------------%
% - 1. Application to MCS
% - 2. Grubbs' Data
% - 3. R implementation
% - 4. Influence measures using R
%---------------------------------------------------------------------------%


\section{Application to MCS} %4.1


Let $\hat{\beta}$ denote the least square estimate of $\beta$
based upon the full set of observations, and let
$\hat{\beta}^{(k)}$ denoted the estimate with the $k^{th}$ case
excluded.




\section{Grubbs' Data} %4.2


For the Grubbs data the $\hat{\beta}$ estimated are
$\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ respectively. Leaving the
fourth case out, i.e. $k=4$ the corresponding estimates are
$\hat{\beta}_{0}^{-4}$ and $\hat{\beta}_{1}^{-4}$




\begin{equation}
Y^{-Q} = \hat{\beta}^{-Q}X^{-Q}
\end{equation}


When considering the regression of case-wise differences and averages, we write $D^{-Q} = \hat{\beta}^{-Q}A^{-Q}$




\newpage


\begin{table}[ht]
\begin{center}
\begin{tabular}{rrrrr}
  \hline
 & F & C & D & A \\
  \hline
1 & 793.80 & 794.60 & -0.80 & 794.20 \\
  2 & 793.10 & 793.90 & -0.80 & 793.50 \\
  3 & 792.40 & 793.20 & -0.80 & 792.80 \\
  4 & 794.00 & 794.00 & 0.00 & 794.00 \\
  5 & 791.40 & 792.20 & -0.80 & 791.80 \\
  6 & 792.40 & 793.10 & -0.70 & 792.75 \\
  7 & 791.70 & 792.40 & -0.70 & 792.05 \\
  8 & 792.30 & 792.80 & -0.50 & 792.55 \\
  9 & 789.60 & 790.20 & -0.60 & 789.90 \\
  10 & 794.40 & 795.00 & -0.60 & 794.70 \\
  11 & 790.90 & 791.60 & -0.70 & 791.25 \\
  12 & 793.50 & 793.80 & -0.30 & 793.65 \\
   \hline
\end{tabular}
\end{center}
\end{table}




\newpage


\begin{equation}
Y^{(k)} = \hat{\beta}^{(k)}X^{(k)}
\end{equation}


Consider two sets of measurements , in this case F and C , with the vectors of case-wise averages $A$ and case-wise differences $D$ respectively. A regression model of differences on averages can be fitted with the view to exploring some characteristics of the data.


When considering the regression of case-wise differences and averages, we write


\begin{equation}
D^{-Q} = \hat{\beta}^{-Q}A^{-Q}
\end{equation}
Let $\hat{\beta}$ denote the least square estimate of $\beta$ based upon the full set of observations, and let $\hat{\beta}^{(k)}$ denoted the estimate with the $k^{th}$ case excluded.


For the Grubbs data the $\hat{\beta}$ estimated are $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ respectively. Leaving the
fourth case out, i.e. $k=4$ the corresponding estimates are $\hat{\beta}_{0}^{-4}$ and $\hat{\beta}_{1}^{-4}$


\begin{equation}
Y^{(k)} = \hat{\beta}^{(k)}X^{(k)}
\end{equation}


Consider two sets of measurements , in this case F and C , with the vectors of case-wise averages $A$ and case-wise differences $D$ respectively. A regression model of differences on averages can be fitted with the view to exploring some characteristics of the data.


\begin{verbatim}
Call: lm(formula = D ~ A)


Coefficients: (Intercept)            A
  -37.51896      0.04656


\end{verbatim}








When considering the regression of case-wise differences and averages, we write


\begin{equation}
D^{-Q} = \hat{\beta}^{-Q}A^{-Q}
\end{equation}






\subsection{Influence measures using R} %4.4
\texttt{R} provides the following influence measures of each observation.


%Influence measures: This suite of functions can be used to compute
%some of the regression (leave-one-out deletion) diagnostics for
%linear and generalized linear models discussed in Belsley, Kuh and
% Welsch (1980), Cook and Weisberg (1982)






\begin{table}[ht]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
 & dfb.1\_ & dfb.A & dffit & cov.r & cook.d & hat \\
  \hline
1 & 0.42 & -0.42 & -0.56 & 1.13 & 0.15 & 0.18 \\
  2 & 0.17 & -0.17 & -0.34 & 1.14 & 0.06 & 0.11 \\
  3 & 0.01 & -0.01 & -0.24 & 1.17 & 0.03 & 0.08 \\
  4 & -1.08 & 1.08 & 1.57 & 0.24 & 0.56 & 0.16 \\
  5 & -0.14 & 0.14 & -0.24 & 1.30 & 0.03 & 0.13 \\
  6 & -0.00 & 0.00 & -0.11 & 1.31 & 0.01 & 0.08 \\
  7 & -0.04 & 0.04 & -0.08 & 1.37 & 0.00 & 0.11 \\
  8 & 0.02 & -0.02 & 0.15 & 1.28 & 0.01 & 0.09 \\
  9 & 0.69 & -0.68 & 0.75 & 2.08 & 0.29 & 0.48 \\
  10 & 0.18 & -0.18 & -0.22 & 1.63 & 0.03 & 0.27 \\
  11 & -0.03 & 0.03 & -0.04 & 1.53 & 0.00 & 0.19 \\
  12 & -0.25 & 0.25 & 0.44 & 1.05 & 0.09 & 0.12 \\
   \hline
\end{tabular}
\end{center}
\end{table}






%-------------------------------------------------------------------------------------------------------%
\chapter{Appendices} % Chapter 5
%---------------------------------------------------------------------------------------------------------%
% Appendices
% - The Hat Matrix (5.1)
% - Sherman Morrison Woodbury Formula (5.2)
% -  Hat Matrix applied to MCS (5.3)
% - Cross Validation (Updating standard deviation) (5.4)
% - Updating Estimates (5.5)
% - Lesaffre's paper (5.6)
%---------------------------------------------------------------------------------------------------------%
%------------------------------------------------------------------------%
\newpage
\section{The Hat Matrix} %5.1


The projection matrix $H$ (also known as the hat matrix), is a
well known identity that maps the fitted values $\hat{Y}$ to the
observed values $Y$, i.e. $\hat{Y} = HY$.


\begin{equation}
H =\quad X(X^{T}X)^{-1}X^{T}
\end{equation}


$H$ describes the influence each observed value has on each fitted
value. The diagonal elements of the $H$ are the `leverages', which
describe the influence each observed value has on the fitted value
for that same observation. The residuals ($R$) are related to the
observed values by the following formula:
\begin{equation}
R = (I-H)Y
\end{equation}


The variances of $Y$ and $R$ can be expressed as:
\begin{eqnarray}
\mbox{var}(Y) = H\sigma^{2} \nonumber\\
\mbox{var}(R) = (I-H)\sigma^{2}
\end{eqnarray}


Updating techniques allow an economic approach to recalculating
the projection matrix, $H$, by removing the necessity to refit the
model each time it is updated. However this approach is known for
numerical instability in the case of down-dating.


\section{Sherman Morrison Woodbury Formula} % 5.2


The `Sherman Morrison Woodbury' Formula is a well known result in
linear algebra;
\begin{equation}
(A+a^{T}B)^{-1} \quad = \quad A^{-1}-
A^{-1}a^{T}(I-bA^{-1}a^{T})^{-1}bA^{-1}
\end{equation}


This result is highly useful for analyzing regression diagnostics,
and for matrices inverses in general. Consider a $p \times p$
matrix $X$, from which a row $x_{i}^{T}$ is to be added or
deleted. \citet{CookWeisberg} sets $A = X^{T}X$, $a=-x_{i}^{T}$
and $b=x_{i}^{T}$, and writes the above equation as


\begin{equation}
(X^{T}X \pm x_{i}x_{i}^{T})^{-1} = \quad(X^{T}X )^{-1} \mp \quad
\frac{(X^{T}X)^{-1}(x_{i}x_{i}^{T}(X^{T}X)^{-1}}{1-x_{i}^{T}(X^{T}X)^{-1}x_{i}}
\end{equation}


The projection matrix $H$ (also known as the hat matrix), is a
well known identity that maps the fitted values $\hat{Y}$ to the
observed values $Y$, i.e. $\hat{Y} = HY$.


\begin{equation}
H =\quad X(X^{T}X)^{-1}X^{T}
\end{equation}


$H$ describes the influence each observed value has on each fitted value. The diagonal elements of the $H$ are the `leverages', which describe the influence each observed value has on the fitted value for that same observation. The residuals ($R$) are related to the observed values by the following formula:
\begin{equation}
R = (I-H)Y
\end{equation}


The variances of $Y$ and $R$ can be expressed as:
\begin{eqnarray}
\mbox{var}(Y) = H\sigma^{2} \nonumber\\
\mbox{var}(R) = (I-H)\sigma^{2}
\end{eqnarray}


Updating techniques allow an economic approach to recalculating the projection matrix, $H$, by removing the necessity to refit the model each time it is updated. However this approach is known for
numerical instability in the case of down-dating.






\subsection{Hat Values for MCS regression}


With A as the averages and D as the casewise differences.
\begin{verbatim}
fit = lm(D~A)
\end{verbatim}


\begin{displaymath}
H = A \left(A^\top  A\right)^{-1} A^\top ,
\end{displaymath}





\printindex
\bibliographystyle{chicago}
\bibliography{DB-txfrbib}


\end{document}
