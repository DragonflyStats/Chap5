\documentclass[Main.tex]{subfiles}
\begin{document}
	

		%---------------------------------------------------------------------------%
\tableofcontents
\section{Introduction to Influence analysis} %1.7
Model diagnostic techniques determine whether or not the distributional assumptions are satisfied, and to assess the influence of unusual observations. In classical linear models model diagnostics have been become a required part of any statistical analysis, and the methods are commonly available in statistical packages and standard textbooks on applied regression. However it has been noted by several papers that model diagnostics do not often accompany LME model analyses.
For linear models for uncorrelated data, it is not necessary to refit the model after removing a data point in order to measure the impact of an observation on the model. The change in fixed effect estimates, residuals, residual sums of squares, and the variance-covariance matrix of the fixed effects can be computed based on the fit to the full data alone. By contrast, in mixed models several important complications arise. Data points can affect not only the fixed effects but also the covariance parameter estimates on which the fixed-effects estimates depend. 

\subsection{What is Influence} %1.1.5


Broadly defined, influence is understood as the ability of a single or multiple data points, through their presence or absence in the data, to alter important aspects of the analysis, yield qualitatively different inferences, or violate assumptions of the statistical model. The goal of influence analysis is not primarily to mark data points for deletion so that a better model fit can be achieved for the reduced data, although this might be a result of influence analysis \citep{schabenberger}.


\subsection{Importance of Influence}
The influence of an observation can be thought of in terms of how much the predicted values for other observations would differ if the observation in question were not included in the model fit.
Likelihood based estimation methods, such as ML and REML, are sensitive to unusual observations. Influence diagnostics are formal techniques that assess the influence of observations on parameter estimates for $\beta$ and $\theta$. A common technique is to refit the model with an observation or group of observations omitted. The basic procedure for quantifying influence is simple as follows:


\begin{enumerate}
	\item Fit the model to the data and obtain estimates of all parameters.
	\item Remove one or more data points from the analysis and compute updated estimates of model parameters.
	\item Based on full- and reduced-data estimates, contrast quantities of interest to determine how the absence of the observations changes the analysis.
\end{enumerate}	
	\newpage
	\section{Introduction and Classical Approach}
	%-----------------------------------------------------------------%
	\subsection{Diagnostic Methods for OLS models}
	% Cook's Distance for OLS models
	% http://www.amstat.org/meetings/jsm/2012/onlineprogram/AbstractDetails.cfm?abstractid=305411
	Influence diagnostics are formal techniques allowing for the identification of observations that exert substantial 
	influence on the estimates of fixed effects and variance covariance parameters. 
	
	The idea of influence diagnostics for a given observation is to quantify the effect of omission of this observation 
	from the data on the results of the model fit. To this aim, the concept of likelihood displacement is used. 
	
	

\citet{cook77} greatly expanded the study of residuals and influence measures. \index{Cook's distance}Cook's Distance , denoted as$D_{(i)}$, is a well known diagnostic technique used in classical linear models, used as an overall measure of the combined impact of the $i$th case of all estimated regression coefficients. Cook's key observation was the effects of deleting each observation in turn could be calculated with little additional computation. That is to say, $D_{(i)}$ can be calculated without fitting a new regression coefficient each time an observation is deleted.  Consequently deletion diagnostics have become an integral part of assessing linear models. 


The focus of this analysis is related to the estimation of point estimates (i.e. regression coefficients). It must be pointed out that the effect on the precision of estimates is separate from the effect on the point estimates. Data points that
have a small \index{Cook's distance}Cook's distance, for example, can still greatly affect hypothesis tests and confidence intervals, if their  influence on the precision of the estimates is large.

As well as individual observations, Cook's distance can be used to analyse the influence of observations in subset $U$ on a vector of parameter estimates \citep{cook77}.
%\section{Effects on fitted and predicted values}
\begin{eqnarray}
\hat{e_{i}}_{(U)} = y_{i} - x\hat{\beta}_{(U)}\\
\delta_{(U)} = \hat{\beta} - \hat{\beta}_{(U)}
\end{eqnarray}
%It uses the same structure for measuring the combined impact of the differences in the estimated regression coefficients when the $k$th case is deleted. 


\subsection{Cook's 1986 paper on Local Influence}%1.7.1
Cook 1986 introduced methods for local influence assessment. These methods provide a powerful tool for examining perturbations in the assumption of a model, particularly the effects of local perturbations of parameters of observations.


\citet{cook77} greatly expanded the study of residuals and influence measures.  Cook's key observation was the effects of deleting each observation in turn could be calculated with little additional computation. That is to say, $D_{(i)}$ can be calculated without fitting a new regression coefficient each time an observation is deleted.  Consequently deletion diagnostics have become an integral part of assessing linear models. Cook proposed a measure that combines the information of leverage and residual of the observation, now known simply as the Cook's Distance. \index{Cook's distance}Cook's Distance , denoted as$D_{(i)}$, is a well known diagnostic technique used in classical linear models, used as an overall measure of the combined impact of the $i-$th case of all estimated regression coefficients.


\subsection{Cook's 1986 paper on Local Influence}%1.7.1
Cook 1986 introduced methods for local influence assessment. These methods provide a powerful tool for examining perturbations in the assumption of a model, particularly the effects of local perturbations of parameters of observations.

The local-influence approach to influence assessment is quitedifferent from the case deletion approach, comparisons are of
interest.
% \subsection{Cook's 1986 paper on Local Influence}%1.7.1
	\citet{cook86} introduces powerful tools for local-influence assessment and examining perturbations in the assumptions of a model. In particular the effect of local perturbations of parameters or observations are examined	
	

	
	The local-influence approach to influence assessment is quitedifferent from the case deletion approach, comparisons are of
	interest.
	
	%---------------------------------------------------------------%
	% We have developed a function in R, which allows performing influence diagnostics for linear mixed effects models 
	% fitted using the lme() function from the nlme package. 
	% The use of the new function is illustrated using data from a randomized clinical trial.
	
	%---------------------------------------------------------------%
	
	\subsection{Influence Diagnostics: Basic Idea and Statistics} %1.1.2
	%http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_mixed_sect024.htm
	
	The general idea of quantifying the influence of one or more observations relies on computing parameter estimates based on all data points, removing the cases in question from the data, refitting the model, and computing statistics based on the change between full-data and reduced-data estimation. 
	

	\subsection{Case Deletion Diagnostics} %1.6
	
	\textbf{CPJ} develops \index{case deletion diagnostics} case deletion diagnostics, in particular the equivalent of \index{Cook's distance} Cook's distance, for diagnosing influential observations when estimating the fixed effect parameters and variance components.
	
	
	
	
	
	
	%--------------------------------------------------------------------------%
	\newpage
	\section{Terminology for Case Deletion diagnostics} %1.8
	
	\citet{preisser} describes two type of diagnostics. When the set consists of only one observation, the type is called
	'observation-diagnostics'. For multiple observations, Preisser describes the diagnostics as 'cluster-deletion' diagnostics.
	
	
	\subsection{Deletion Diagnostics}
	
	Since the pioneering work of Cook in 1977, deletion measures have been applied to many statistical models for identifying influential observations.
	
	Deletion diagnostics provide a means of assessing the influence of an observation (or groups of observations) on inference on the estimated parameters of LME models.
	
	Data from single individuals, or a small group of subjects may influence non-linear mixed effects model selection. Diagnostics routinely applied in model building may identify such individuals, but these methods are not specifically designed for that purpose and are, therefore, not optimal. We describe two likelihood-based diagnostics for identifying individuals that can influence the choice between two competing models.
	
	Case-deletion diagnostics provide a useful tool for identifying influential observations and outliers.
	
	The computation of case deletion diagnostics in the classical model is made simple by the fact that estimates of $\beta$ and $\sigma^2$, which exclude the ith observation, can be computed without re-fitting the model. Such update formulas are available in the mixed model only if you assume that the covariance parameters are not affected by the removal of the observation in question. This is rarely a reasonable assumption.
	
	\subsubsection{Effects on fitted and predicted values}
	\begin{equation}
	\hat{e_{i}}_{(U)} = y_{i} - x\hat{\beta}_{(U)}
	\end{equation}
	\newpage
	
%===================================================================================================
\subsection{Influence Diagnostics: Basic Idea and Statistics} %1.1.2
Broadly defined, ``\textit{influence}” is understood as the ability of a single or multiple data points, through their presence or absence in the data, to alter important aspects of the analysis, yield qualitatively different inferences, or
		violate assumptions of the statistical model. 
		
		
		The goal of influence analysis is not primarily to mark data
		points for deletion so that a better model fit can be achieved for the reduced data, although this might be a
		result of influence analysis. The goal is rather to determine which cases are influential and the manner in
		which they are important to the analysis. Outliers, for example, may be the most noteworthy data points in
		an analysis. They can point to a model breakdown and lead to development of a better model.
		
		%http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_mixed_sect024.htm
		
		The general idea of quantifying the influence of one or more observations relies on computing parameter estimates based on all data points, removing the cases in question from the data, refitting the model, and computing statistics based on the change between full-data and reduced-data estimation. 

			%---------------------------------------------------------------------------%
			\newpage

\subsection{What is Influence} %1.1.5

Broadly defined, influence is understood as the ability of a single or multiple data points, through their presence or absence in the data, to alter important aspects of the analysis, yield qualitatively different inferences, or violate assumptions of the statistical model. The goal of influence analysis is not primarily to mark data
points for deletion so that a better model fit can be achieved for the reduced data, although this might be a result of influence analysis \citep{schabenberger}.



%---------------------------------------------------------------------------%
\newpage



			\section{Influence analysis for LME Models} %1.7

				
				Model diagnostic techniques, well established for classical models, have since been adapted for use with linear mixed effects models. Diagnostic techniques for LME models are inevitably more difficult to implement, due to the increased complexity.
					
			Likelihood based estimation methods, such as ML and REML, are sensitive to unusual observations. Influence diagnostics are formal techniques that assess the influence of observations on parameter estimates for $\beta$ and $\theta$. A common technique is to refit the model with an observation or group of observations omitted.
			
			\citet{west} examines a group of methods that examine various aspects of influence diagnostics for LME models.
			For overall influence, the most common approaches are the `likelihood distance' and the `restricted likelihood distance'.
			
	
	%--------------------------------------------------------------------------------------------%
	\subsection{Influence Analysis for LME Models} %1.1.3
	The linear mixed effects model is a useful methodology for fitting a wide range of models. However, linear mixed effects models are known to be sensitive to outliers. \citet{CPJ} advises that identification of outliers is necessary before conclusions may be drawn from the fitted model.
	
	Standard statistical packages concentrate on calculating and testing parameter estimates without considering the diagnostics of the model.The assessment of the effects of perturbations in data, on the outcome of the analysis, is known as statistical influence analysis. Influence analysis examines the robustness of the model. Influence analysis methodologies have been used extensively in classical linear models, and provided the basis for methodologies for use with LME models.
	Computationally inexpensive diagnostics tools have been developed to examine the issue of influence \citep{Zewotir}.
	%Studentized residuals, error contrast matrices and the inverse of the response variance covariance matrix are regular components of these tools.	
		
		Studentized residuals, error contrast matrices and the inverse of the response variance covariance matrix are regular components of these tools.
		
		Influence arises at two stages of the LME model. Firstly when $V$ is estimated by $\hat{V}$, and subsequent
		estimations of the fixed and random regression coefficients $\beta$ and $u$, given $\hat{V}$.
		
		%--------------------------------------------------------------%
		
		%---------------------------------------------------------------------------%
	
		
				\subsection{Computation Matters}
				Key to the implementations of influence diagnostics in the MIXED procedure is the attempt to quantify influence, where possible, by drawing on the basic definitions of the various statistics in the classical linear	model. 
				
				On occasion, quantification is not possible. Assume, for example, that a data point is removed
				and the new estimate of the G matrix is not positive definite. This may occur if a variance component estimate now falls on the boundary of the parameter space. Thus, it may not be possible to compute certain influence statistics comparing the full-data and reduced-data parameter estimates. However, knowing that a new singularity was encountered is important qualitative information about the data point’s influence on	the analysis.
		%---------------------------------------------------------------------------%
		\newpage
		\subsection{Extension of techniques to LME Models} %1.2
		
		Model diagnostic techniques, well established for classical models, have since been adapted for use with linear mixed effects models.Diagnostic techniques for LME models are inevitably more difficult to implement, due to the increased complexity.
		
		Beckman, Nachtsheim and Cook (1987) \citet{Beckman} applied the \index{local influence}local influence method of Cook (1986) to the analysis of the linear mixed model.
		
		While the concept of influence analysis is straightforward, implementation in mixed models is more complex. Update formulae for fixed effects models are available only when the covariance parameters are assumed to be known.
		
		If the global measure suggests that the points in $U$ are influential, the nature of that influence should be determined. In particular, the points in $U$ can affect the following
		
		\begin{itemize}
			\item the estimates of fixed effects,
			\item the estimates of the precision of the fixed effects,
			\item the estimates of the covariance parameters,
			\item the estimates of the precision of the covariance parameters,
			\item fitted and predicted values.
		\end{itemize}


				%==================================================================================================== %
				\subsection{Analyzing Influence in LME models}
				``\textit{Influence}” is defined by \citet{schab} as ``the ability of a single or multiple data points, through their presence
				or absence in the data, to alter important aspects of the analysis, yield qualitatively different inferences, or
				violate assumptions of the statistical model". The goal of influence analysis is rather to identify influential cases and the manner in
				which they are important to the analysis. A consequence of this that cases may be to mark data
				points for deletion so that a better model fit can be achieved for the reduced data \citep{schab}.  
				
				% MOVE BACK TO START
				%  Outliers, for example, may be the most noteworthy data points in
				%  an analysis. They can point to a model breakdown and lead to development of a better model.
				
				
				\citet{schab} considers several important aspects of the use and implementation of influence measures in LME models. \textit{schabenberger} notes that it is not always possible to
				derive influence statistics necessary for comparing full- and reduced-data parameter estimates. 
				
				\citet{schab} describes a simple procedure for quantifying influence. Firstly a model should be fitted to the data, and
				estimates of the parameters should be obtained. The second step is that either single or multiple data points, specifically outliers,
				should be omitted from the analysis, with the original parameter estimates being updated. This is known as `\textit{leave one out \ leave k out}' analysis. The final step of the procedure is comparing the 	sets of estimates computed from the entire and reduced data sets to determine whether the absence of observations changed the
				analysis.		

		
		\subsection{Influence in LME models (schab)}
		Likelihood based estimation methods, such as ML and REML, are sensitive to unusual observations. Influence diagnostics are formal techniques that assess the influence of observations on parameter estimates for $\beta$ and $\theta$. A common technique is to refit the model with an observation or group of observations omitted.\citet{west} examines a group of methods that examine various aspects of influence diagnostics for LME models.
		For overall influence, the most common approaches are the `likelihood distance' and the `restricted likelihood distance'.
		
		\emph{schab} examines the use and implementation of influence measures in LME models.
		
		Influence is understood to be the ability of a single or multiple
		data points, through their presences or absence in the data, to
		alter important aspects of the analysis, yield qualitatively
		different inferences, or violate assumptions of the statistical
		model (\textit{schabenberger}).
		
		Outliers are the most noteworthy data points in an analysis, and
		an objective of influence analysis is how influential they are,
		and the manner in which they are influential.
		
		\emph{schab} describes a simple procedure for quantifying
		influence. Firstly a model should be fitted to the data, and
		estimates of the parameters should be obtained. The second step is
		that either single of multiple data points, specifically outliers,
		should be omitted from the analysis, with the original parameter
		estimates being updated. 
		
		This is known as `\textit{leave one out \ leave k
			out}' analysis. The final step of the procedure is comparing the
		sets of estimates computed from the entire and reduced data sets
		to determine whether the absence of observations changed the
		analysis.
		
		\textit{schabenberger} notes that it is not always possible to
		derive influence statistics necessary for comparing full- and
		reduced-data parameter estimates. 
		
		%
		%\begin{abstract}
		%	\noindent This paper reviews the use of diagnostic measures for LME models in SAS. This text has been widely cited by texts that don't deal with SAS implementations.
		%\end{abstract}
		%
		

		
		
		%==================================================================================================== %
		
		In recent years, mixed models have become invaluable tools in the analysis of experimental and observational
		data. In these models, more than one term can be subject to random variation. Mixed model
		technology enables you to analyze complex experimental data with hierarchical random processes, temporal,
		longitudinal, and spatial data, to name just a few important applications. 
		%
		%\subsection{Stating the LME Model}
		%The general linear mixed
		%model is
		%\[
		%Y = X\beta + Zu + \varepsilon\]
		%where Y is a $(n\times1)$ vector of observed data, X is an $(n\times p)$ fixed-effects design or regressor matrix of rank
		%k, Z is a $(n \times g)$ random-effects design or regressor matrix, $u$ is a $(g \times 1)$ vector of random effects, and $\varepsilon$ is
		%an $(n\times1)$ vector of model errors (also random effects). The distributional assumptions made by the MIXED
		%procedure are as follows: γ is normal with mean 0 and variance G; $\varepsilon$ is normal with mean 0 and variance
		%R; the random components $u$ and $\varepsilon$ are independent. Parameters of this model are the fixed-effects β and
		%all unknowns in the variance matrices G and R. The unknown variance elements are referred to as the
		%covariance parameters and collected in the vector $theta$.
		%===========================================================================%
		
		\emph{schab} remarks that the concept of critiquing the model-data agreement applies in mixed models in the same way as in linear
		fixed-effects models. In fact, because of the more complex model structure, you can argue that model and
		data diagnostics are even more important. For example, you are not only concerned with capturing the
		important variables in the model. You are also concerned with ``distributing” them correctly between the
		fixed and random components of the model. The mixed model structure presents unique and interesting
		challenges that prompt us to reexamine the traditional ideas of influence and residual analysis.
		%==========================================================================%
		%This paper presents the extension of traditional tools and statistical measures for influence and residual
		%analysis to the linear mixed model and demonstrates their implementation in the MIXED procedure (experimental
		%features in SAS 9.1). The remainder of this paper is organized as follows. The “Background” section
		%briefly discusses some mixed model estimation theory and the challenges to model diagnosis that result
		%from it.
		
		%	 The diagnostics implemented in the MIXED procedure are discussed in the “Residual Diagnostics
		%	in the MIXED Procedure” section (page 3) and the “Influence Diagnostics in the MIXED Procedure” section
		%	(page 5). The syntax options and suboptions you use to request the various diagnostics are briefly sketched
		%	in the “Syntax” section (page 9). The presentation concludes with an example.
		%	
		%	
		%====================================================================================================================%
		
\newpage
\section{Overall Influence and Iterative Influence Analysis}
			An overall influence statistic measures the change in the objective function being minimized. For example, in
			OLS regression, the residual sums of squares serves that purpose. In linear mixed models fit by
			\index{maximum likelihood} maximum likelihood (ML) or \index{restricted maximum likelihood} restricted maximum likelihood (REML), an overall influence measure is the \index{likelihood distance} likelihood distance [Cook and Weisberg ].
				
	%---------------------------------------------------------------------------%
	
	\subsection{Overall Influence}
	An overall influence statistic measures the change in the objective function being minimized. For example, in
	OLS regression, the residual sums of squares serves that purpose. In linear mixed models fit by
	\index{maximum likelihood} maximum likelihood (ML) or \index{restricted maximum likelihood} restricted maximum likelihood (REML), an overall influence measure is the \index{likelihood distance} likelihood distance [Cook and Weisberg ].
		
	\subsection{Iterative Influence Analysis}
	
	
	
	\citet{schabenberger} describes the choice between \index{iterative influence analysis} iterative influence analysis and \index{non-iterative influence analysis} non-iterative influence analysis.
		
		
		
				\subsection{Iterative Influence Analysis}
				
				%----schabenberger page 8
				For linear models, the implementation of influence analysis is straightforward.
				However, for LME models, the process is more complex. Update formulas for the fixed effects are available only when the covariance parameters are assumed to be known. A measure of total influence requires updates of all model parameters.
				This can only be achieved in general is by omitting observations, then refitting the model.
				
				\citet{schabenberger} describes the choice between \index{iterative influence analysis} iterative influence analysis and \index{non-iterative influence analysis} non-iterative influence analysis.
				

	

\subsection{Iterative and non-iterative influence analysis} %1.13
\citet{schabenberger} highlights some of the issue regarding implementing mixed model diagnostics.


A measure of total influence requires updates of all model parameters.


however, this doesnt increase the procedures execution time by the same degree.

\newpage




			
				

		
		%============================================================================================================================ %
		


		
	
		%---------------------------------------------------------------------------%
		\subsection{Local Influence}
		% % Beckman, Nachtsheim and Cook (1987) 
		\citet{Beckman} applied the \index{local influence}local influence method of Cook (1986) to the analysis of the LME model.
		While the concept of influence analysis is straightforward, implementation in mixed models is more complex. Update formulae for fixed effects models are available only when the covariance parameters are assumed to be known.
		
		If the global measure suggests that the points in $U$ are influential, the nature of that influence should be determined. In particular, the points in $U$ can affect the following
		
		\begin{itemize}
			\item the estimates of fixed effects,
			\item the estimates of the precision of the fixed effects,
			\item the estimates of the covariance parameters,
			\item the estimates of the precision of the covariance parameters,
			\item fitted and predicted values.
		\end{itemize}
	

		

		
			
			
			
\newpage		
			\section{A Procedure for Quantifying Influence}  %1.1.6

		
		The basic procedure for quantifying influence is simple:
		
		\begin{enumerate}
			\item Fit the model to the data and obtain estimates of all parameters.
			\item Remove one or more data points from the analysis and compute updated estimates of model parameters.
			\item Based on full- and reduced-data estimates, contrast quantities of interest to determine how the absence
			of the observations changes the analysis.
		\end{enumerate}
		We use the subscript (U) to denote quantities obtained without the observations in the set U. For example,
		%βb
		(U) denotes the fixed-effects “\textit{\textbf{leave-U-out}}” estimates. Note that the set U can contain multiple observations.
		
		
		%===================================================================================
		If the global measure suggests that the points in U are influential, you should next determine the nature of
		that influence. In particular, the points can affect
		\begin{itemize}
			\item the estimates of fixed effects
			\item the estimates of the precision of the fixed effects
			\item the estimates of the covariance parameters
			\item the estimates of the precision of the covariance parameters
			\item fitted and predicted values
		\end{itemize}
		
		It is important to further decompose the initial finding to determine whether data points are actually troublesome.
		Simply because they are influential “somehow”, should not trigger their removal from the analysis or
		a change in the model. For example, if points primarily affect the precision of the covariance parameters
		without exerting much influence on the fixed effects, then their presence in the data may not distort hypothesis
		tests or confidence intervals about $\beta$.
		%They will only do so if your inference depends on an estimate of the
		%precision of the covariance parameter estimates, as is the case for the Satterthwaite and Kenward-Roger
		%degrees of freedom methods and the standard error adjustment associated with the DDFM=KR option.

%================================================ %
		
			\section{Influence Statistics for LME models} %1.1.4
			Influence statistics can be coarsely grouped by the aspect of estimation that is their primary target:
			\begin{itemize}
				\item overall measures compare changes in objective functions: (restricted) likelihood distance (Cook and Weisberg 1982, Ch. 5.2)
				\item influence on parameter estimates: Cook's  (Cook 1977, 1979), MDFFITS (Belsley, Kuh, and Welsch 1980, p. 32)
				\item influence on precision of estimates: CovRatio and CovTrace
				\item influence on fitted and predicted values: PRESS residual, PRESS statistic (Allen 1974), DFFITS (Belsley, Kuh, and Welsch 1980, p. 15)
				\item outlier properties: internally and externally studentized residuals, leverage
			\end{itemize}


	
	\subsection{Cook's Distance} %2.4.1
	\begin{itemize}
		\item For variance components $\gamma$
	\end{itemize}
	
	Diagnostic tool for variance components
	\[ C_{\theta i} =(\hat(\theta)_{[i]} - \hat(\theta))^{T}\mbox{cov}( \hat(\theta))^{-1}(\hat(\theta)_{[i]} - \hat(\theta))\]
	
%---------------------------------------------------------------------------%
\subsection{Variance Ratio} %2.4.2
\begin{itemize}
	\item For fixed effect parameters $\beta$.
\end{itemize}

	
	\subsection{Cook-Weisberg statistic} %2.4.3
	\begin{itemize}
		\item For fixed effect parameters $\beta$.
	\end{itemize}
	\subsection{Zewotir Measures of Influence in LME Models}%2.2
	%Zewotir page 161
	\citet{Zewotir} describes a number of approaches to model diagnostics, investigating each of the following;
	\begin{itemize}
		\item Variance components
		\item Fixed effects parameters
		\item Prediction of the response variable and of random effects
		\item likelihood function
	\end{itemize}
	
%-------------------------------------------------------------------------------------------------------------------------------------%
\subsection{Zewotir Measures of Influence in LME Models}%2.2
%Zewotir page 161
\citet{Zewotir} describes a number of approaches to model diagnostics, investigating each of the following;
\begin{itemize}
	\item Variance components
	\item Fixed effects parameters
	\item Prediction of the response variable and of random effects
	\item likelihood function
\end{itemize}

\citet{Zewotir} lists several established methods of analyzing influence in LME models. These methods include \begin{itemize}
	\item Cook's distance for LME models,
	\item \index{likelihood distance} likelihood distance,
	\item the variance (information) ration,
	\item the \index{Cook-Weisberg statistic} Cook-Weisberg statistic,
	\item the \index{Andrews-Prebigon statistic} Andrews-Prebigon statistic.
\end{itemize}


	
	\subsection{Andrews-Pregibon statistic} %2.4.4
	\begin{itemize}
		\item For fixed effect parameters $\beta$.
	\end{itemize}
	The Andrews-Pregibon statistic $AP_{i}$ is a measure of influence based on the volume of the confidence ellipsoid.
	The larger this statistic is for observation $i$, the stronger the influence that observation will have on the model fit.


	
	\newpage
	\subsubsection{Random Effects}
	
	A large value for $CD(u)_i$ indicates that the $i-$th observation is influential in predicting random effects.
	

	
	%--------------------------------------------------------------%
	\newpage
	\subsection{Computation and Notation } %2.3
	with $\boldsymbol{V}$ unknown, a standard practice for estimating $\boldsymbol{X \beta}$ is the estime the variance components $\sigma^2_j$,
	compute an estimate for $\boldsymbol{V}$ and then compute the projector matrix $A$, $\boldsymbol{X \hat{\beta}}  = \boldsymbol{AY}$.
	
	
	Zewotir remarks that $\boldsymbol{D}$ is a block diagonal with the $i-$th block being $u \boldsymbol{I}$


	

\subsection{Cook's Distance}
\begin{itemize}
	\item For variance components $\gamma$: $CD(\gamma)_i$,
	\item For fixed effect parameters $\beta$: $CD(\beta)_i$,
	\item For random effect parameters $\boldsymbol{u}$: $CD(u)_i$,
	\item For linear functions of $\hat{beta}$: $CD(\psi)_i$
\end{itemize}
	Diagnostic tool for variance components
	\[ C_{\theta i} =(\hat(\theta)_{[i]} - \hat(\theta))^{T}\mbox{cov}( \hat(\theta))^{-1}(\hat(\theta)_{[i]} - \hat(\theta))\]

\newpage
\subsubsection{Random Effects}

A large value for $CD(u)_i$ indicates that the $i-$th observation is influential in predicting random effects.

\subsubsection{linear functions}

$CD(\psi)_i$ does not have to be calculated unless $CD(\beta)_i$ is large.


\subsection{Information Ratio}




\printindex
\bibliographystyle{chicago}
\bibliography{DB-txfrbib}


\end{document}